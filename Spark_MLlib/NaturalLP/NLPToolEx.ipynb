{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('NLPTools').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "from pyspark.ml.feature import Tokenizer,RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,udf\n",
    "from pyspark.sql.types import IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataFrame = spark.createDataFrame([\n",
    "    (1,'Hi I heard About Spark is a Good Language'),\n",
    "    (2,'My name is Debabrata and I am pursing my Masters'),\n",
    "    (3,'Language,Java,C,Python,Spark,Good,Masters')\n",
    "],['id','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  1|Hi I heard About ...|\n",
      "|  2|My name is Debabr...|\n",
      "|  3|Language,Java,C,P...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_dataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence',outputCol='Words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_tokenizer = RegexTokenizer(inputCol='sentence',outputCol='RegWords',pattern='\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = udf(lambda words:len(words),IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenizer.transform(my_dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               Words|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|Hi I heard About ...|[hi, i, heard, ab...|\n",
      "|  2|My name is Debabr...|[my, name, is, de...|\n",
      "|  3|Language,Java,C,P...|[language,java,c,...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               Words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  1|Hi I heard About ...|[hi, i, heard, ab...|     9|\n",
      "|  2|My name is Debabr...|[my, name, is, de...|    10|\n",
      "|  3|Language,Java,C,P...|[language,java,c,...|     1|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_data.withColumn('tokens',count_tokens(col('Words'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regtokenized_data = req_tokenizer.transform(my_dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data=regtokenized_data.withColumn('tokens',count_tokens(col('RegWords')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|            RegWords|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  1|Hi I heard About ...|[hi, i, heard, ab...|     9|\n",
      "|  2|My name is Debabr...|[my, name, is, de...|    10|\n",
      "|  3|Language,Java,C,P...|[language, java, ...|     7|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reg_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StopWords Removal\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='RegWords',outputCol='filteredWords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data=remover.transform(reg_data).withColumn('Aftertokens',count_tokens(col('filteredWords')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nGrams\n",
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = NGram(n=2,inputCol='filteredWords',outputCol='Ngram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|Ngram                                                                    |\n",
      "+-------------------------------------------------------------------------+\n",
      "|[hi heard, heard spark, spark good, good language]                       |\n",
      "|[name debabrata, debabrata pursing, pursing masters]                     |\n",
      "|[language java, java c, c python, python spark, spark good, good masters]|\n",
      "+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram.transform(filtered_data).select('Ngram').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_data = ngram.transform(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sentence: string (nullable = true)\n",
      " |-- RegWords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tokens: integer (nullable = true)\n",
      " |-- filteredWords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Aftertokens: integer (nullable = true)\n",
      " |-- Ngram: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#termFrequenct, IDF, \n",
    "from pyspark.ml.feature import HashingTF,IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_tf = HashingTF(inputCol='filteredWords',outputCol='HashingTF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizedData = hashing_tf.transform(ngram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|       filteredWords|           HashingTF|\n",
      "+--------------------+--------------------+\n",
      "|[hi, heard, spark...|(262144,[49304,73...|\n",
      "|[name, debabrata,...|(262144,[31617,35...|\n",
      "|[language, java, ...|(262144,[28698,55...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData.select('filteredWords','HashingTF').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol='HashingTF',outputCol='IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_model = idf.fit(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_data = idf_model.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|IDF                                                                                                                                                                                               |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(262144,[49304,73197,113432,116836,234657],[0.6931471805599453,0.6931471805599453,0.28768207245178085,0.28768207245178085,0.28768207245178085])                                                   |\n",
      "|(262144,[31617,35119,79364,243021],[0.6931471805599453,0.6931471805599453,0.28768207245178085,0.6931471805599453])                                                                                |\n",
      "|(262144,[28698,55551,79364,97597,113432,116836,234657],[0.6931471805599453,0.6931471805599453,0.28768207245178085,0.6931471805599453,0.28768207245178085,0.28768207245178085,0.28768207245178085])|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf_data.select('IDF').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol='filteredWords',outputCol='CountVectorized',vocabSize=3,minDF=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = cv.fit(idf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data=cv_model.transform(idf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-------------------------+\n",
      "|filteredWords                                    |CountVectorized          |\n",
      "+-------------------------------------------------+-------------------------+\n",
      "|[hi, heard, spark, good, language]               |(3,[0,1],[1.0,1.0])      |\n",
      "|[name, debabrata, pursing, masters]              |(3,[2],[1.0])            |\n",
      "|[language, java, c, python, spark, good, masters]|(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "+-------------------------------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_data.select('filteredWords','CountVectorized').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
